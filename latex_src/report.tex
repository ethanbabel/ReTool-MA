\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{natbib}
\bibliographystyle{plainnat}

\title{Machine Learning: Reinforcement Learning Final Project Report}

\author{Ethan Babel}

\begin{document}

\maketitle

\begin{abstract}
  Placeholder for the project abstract. Summarize the research motivation, the ReTool-MA contribution, and key findings once the work is complete.
\end{abstract}

\section{Introduction}
Placeholder for the introduction. Outline the motivation, problem setting, and the roadmap for the rest of the report.
\citep{marti2025}

\section{ReTool: Paper Summary}
This section summarizes the ReTool: Reinforcement Learning for Strategic Tool Use in LLMs paper 
\citep{feng2025retoolreinforcementlearningstrategic}, which introduces a reinforcement learning (RL) 
framework that tightly integrates code interpreter (CI) tool use into the reasoning process of large 
language models (LLMs).

\subsection{ReTool Motivation}
Reasoning-focused LLMs trained with RL perform well on text-only
tasks but struggle on problems that demand precise computation, symbolic manipulation, 
or geometric reasoning. Purely textual reasoning often accumulates errors over long horizons, 
whereas code interpreters can provide exact numeric validation and programmatic exploration. 
Recent research has explored both prompting and supervised fine-tuning (SFT) based approaches to 
encourage tool use in LLMs, however these efforts have shown to generalize quite poorly. 
ReTool targets this gap by training an LLM to decide when and how to call tools, utilize code 
execution with natural language reasoning, and refine its strategy through outcome-driven RL.

\subsection{ReTool Methodology}
ReTool follows a two-stage pipeline. First, a cold-start SFT stage uses 
a curated synthetic dataset of code-augmented reasoning traces to bootstrap 
tool-calling competence. Second, an PPO-based RL stage rolls out trajectories 
that interleave natural language reasoning with code execution inside a sandboxed CI. 
The policy receives interpreter outputs (including errors) and continues generation, 
enabling iterative refinement and code self-correction.

During rollouts, code snippets are delimited with \texttt{<code>} tags. When a closing 
tag appears, the snippet is executed and feedback is inserted in \texttt{<interpreter>} 
tags before generation resumes. The reward is a simple rule-based accuracy signal 
(+1 for correct answers in a required format such as \verb|\\boxed{}|, -1 otherwise), 
avoiding reward hacking while encouraging diverse strategies. 

\subsection{ReTool Results}
On the AIME 2024 and 2025 math benchmarks, ReTool substantially outperforms text-only RL 
baselines and other public tool-use systems. With Qwen2.5--32B-Instruct, ReTool achieves 
67.0\% (AIME 2024) and 49.3\% (AIME 2025) after only 400 training steps, compared to 40.0\% 
and 36.7\% for a text-based RL baseline that needs over 1{,}000 steps. Using a stronger 
backbone (DeepSeek-R1-Distill-Qwen-32B), ReTool reaches 72.5\% and 54.3\% on AIME 2024 and 
2025, surpassing OpenAI's o1-preview by 27.9 points on AIME 2024. The cold-start model alone 
matches text-based RL performance (around 41\% on AIME 2024), indicating that tool-aware SFT 
is a strong starting point and RL delivers the remaining gains.

\subsection{ReTool Output Behavioral Analysis}
ReTool exhibits emergent tool-use behaviors during RL. Response length drops by roughly 40\% 
compared to pre-RL baselines, as code replaces verbose textual computation. The share of 
responses containing code grows to nearly 98\%, while simultaneously code snippets become 
longer and more complex. Most notably, interpreter feedback drives code self-correction. 
The model can detect execution errors, revise functions, and re-run code until it succeeds, 
reflecting a learned strategy for adaptive tool use rather than simple imitation.

\section{ReTool-MA Methodology}
Building on the ReTool observations, our system explicitly decomposes the policy into 
dedicated Planner, Executor, and Verifier roles implemented with MARTI's 
multi-agent workflow engine. Each agent is backed by a Qwen2.5 reasoning model, but the 
workflow orchestrates their interaction through a directed graph, tool calls, and 
role-specific rewards.

\subsection{Role Decomposition and MARTI Integration}
We instantiate a custom workflow 
\texttt{planner} $\rightarrow$ \texttt{executor} $\rightarrow$ \texttt{verifier}
inside MARTI's asynchronous DAG runtime. The Planner receives the raw math problem and 
must emit a structured JSON plan containing free-form reasoning, a high-level code sketch, 
and the anticipated numerical quantity. The Executor is prompted with the Planner JSON and 
is only allowed to emit executable Python. Finally, the Verifier consumes the original problem, 
Planner justification, and the Executor's code transcript to decide whether the trajectory should 
terminate or request revisions.

The workflow returns a full trajectory trace, including intermediate prompts, code snippets, 
tool metadata, and a compact metrics object. During offline evaluation (and later RL training) we 
log aggregate summaries such as accuracy, average number of tool calls, and code pass rates split 
by correct vs.\ incorrect final answers. These summaries are written to \texttt{results.json} and 
\texttt{summary.json}, mirroring the statistical diagnostics that motivated ReTool.

\subsection{Local Code Execution and Instrumentation}
To avoid dependence on remote execution services, we implemented a lightweight 
\texttt{local\_python} tool that drops code into a temporary directory, runs the repository 
Python interpreter with strict timeouts, captures stdout/stderr, and cleans up artifacts. The tool 
is registered via MARTI's \texttt{ToolManager}, so both inference and RL use the exact same sandbox. 
Each tool call contributes to a global metrics collector (success/failure counts and latency) and the 
workflow emits a per-problem diagnostic bundle:
\[
\text{code\_metrics} = 
\left\{\text{attempts}, \text{passes}, \text{failures}, \text{pass rate conditioned on correctness}\right\},
\]
which is later consumed by our evaluator and report.

\subsection{Reward Shaping for Role-Specific Behaviors}
The base ReTool reward is a sparse correctness indicator provided by the math grader. To inject 
information earlier in the trajectory we added small shaping terms for each role:
\begin{align}
r_{\text{plan}} &= \alpha_{\text{succ}}\,\mathbb{1}\{\text{tool success}\}
                  + \alpha_{\text{spec}}\,\mathbb{1}\{\text{tool output matches expected spec}\}
                  - \alpha_{\text{fail}}\,\mathbb{1}\{\text{tool failure}\}, \\
r_{\text{exec}} &= \beta_{\text{succ}}\,\mathbb{1}\{\text{tool success}\}
                  + \beta_{\text{align}}\,\mathcal{O}(\text{plan},\text{code})
                  - \beta_{\text{fail}}\,\mathbb{1}\{\text{tool failure}\}, \\
r_{\text{ver}}  &= r_{\text{final}}
                  - \gamma_{\text{miss}}\,\mathbb{1}\{\text{accepted wrong tool output}\}
                  - \gamma_{\text{reject}}\,\mathbb{1}\{\text{rejected correct tool output}\}.
\end{align}
The overlap term $\mathcal{O}(\text{plan},\text{code}) \in [0,1]$ measures lexical agreement 
between the Planner's \texttt{code\_plan} and the Executor's snippet, so the Executor is 
rewarded for adhering to specifications rather than blindly solving the problem. Reward values are 
clamped to $[-1,1]$ so that shaping remains a gentle nudge around the sparse terminal signal. The 
intuitive effect is that the Planner is nudged to produce executable specifications, the Executor 
to write code that mirrors those specifications, and the Verifier to double-check whether the 
tool trace actually solves the problem before committing to a final answer.

In preliminary AMC trials the shaped rewards immediately highlight the failure mode we observed in 
ReTool: the Executor achieves a perfect tool pass rate even on incorrect trajectories, but the 
Verifier receives explicit penalties when it accepts those incorrect tool outputs. This should 
encourage the Verifier to demand revisions in future RL training rather than rubber-stamping every 
tool call, aligning with the cognitive separation goals of ReTool-MA.

\bibliography{references}

\end{document}
